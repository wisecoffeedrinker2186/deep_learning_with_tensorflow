{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1UnmLGCPU1c4eMZjrET9xZBHMEKP4bGGo","timestamp":1694574499186}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Tutorial on LangChain**\n","\n","### Dr. Santosh Chapaneri\n","### Sr. Data Scientist, Wolters Kluwer\n","\n","### LLM 101 (Hands-on)\n","### MPSTME, Sep 2023"],"metadata":{"id":"47DLL8LWZ9sR"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"En3KtETcvkdk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1694575726016,"user_tz":-330,"elapsed":7662,"user":{"displayName":"Atindra Jayakar","userId":"03118470520503143272"}},"outputId":"728305c0-3844-4eb7-c61b-05d7c7150a85"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting langchain\n","  Downloading langchain-0.0.287-py3-none-any.whl (1.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting huggingface_hub\n","  Downloading huggingface_hub-0.17.1-py3-none-any.whl (294 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.8/294.8 kB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.20)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.5)\n","Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n","Collecting dataclasses-json<0.6.0,>=0.5.7 (from langchain)\n","  Downloading dataclasses_json-0.5.14-py3-none-any.whl (26 kB)\n","Collecting langsmith<0.1.0,>=0.0.21 (from langchain)\n","  Downloading langsmith-0.0.36-py3-none-any.whl (37 kB)\n","Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.5)\n","Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n","Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.12)\n","Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n","Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.12.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.5.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (23.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.2.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n","Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n","  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n","  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.7.22)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n","Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain)\n","  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n","Installing collected packages: mypy-extensions, marshmallow, typing-inspect, langsmith, huggingface_hub, dataclasses-json, langchain\n","Successfully installed dataclasses-json-0.5.14 huggingface_hub-0.17.1 langchain-0.0.287 langsmith-0.0.36 marshmallow-3.20.1 mypy-extensions-1.0.0 typing-inspect-0.9.0\n"]}],"source":["# install\n","! pip install langchain huggingface_hub"]},{"cell_type":"markdown","source":["For Hugging Face models we need a Hugging Face Hub API token.\n","\n","We can find this by first getting an account at [HuggingFace.co](https://huggingface.co/) and clicking on our profile in the top-right corner > click *Settings* > click *Access Tokens* > click *New Token* > set *Role* to *write* > *Generate* > copy and paste the token below:"],"metadata":{"id":"0Yc2oJErv_FJ"}},{"cell_type":"code","source":["import os\n","os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\""],"metadata":{"id":"LmvkshEKv4fZ","executionInfo":{"status":"ok","timestamp":1694575726017,"user_tz":-330,"elapsed":9,"user":{"displayName":"Atindra Jayakar","userId":"03118470520503143272"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["from langchain import HuggingFaceHub\n","\n","# Repo from HuggingFaceHub\n","flan_t5 = HuggingFaceHub(repo_id = \"google/flan-t5-xxl\", # model to use\n","                         model_kwargs={\"temperature\":0.1, \"max_new_tokens\":200})  # temp -> param to control amt of randomness"],"metadata":{"id":"K30b5i0uw5II","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1694576351597,"user_tz":-330,"elapsed":748,"user":{"displayName":"Atindra Jayakar","userId":"03118470520503143272"}},"outputId":"bb1ef853-4583-4c0f-aba3-1f69afb30d1e"},"execution_count":62,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:127: FutureWarning: '__init__' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '0.19.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n","  warnings.warn(warning_message, FutureWarning)\n"]}]},{"cell_type":"markdown","source":["# **Simple Prompt**"],"metadata":{"id":"ZJPIx4dWyfO4"}},{"cell_type":"code","source":["prompt = \"\"\"Answer the question based on the context below. If the\n","question cannot be answered using the information provided answer\n","with \"I don't know\".\n","\n","Context: Large Language Models (LLMs) are the latest models used in NLP.\n","Their superior performance over smaller models has made them incredibly\n","useful for developers building NLP enabled applications. These models\n","can be accessed via Hugging Face's `transformers` library, via OpenAI\n","using the `openai` library, and via Cohere using the `cohere` library.\n","\n","Question: Which libraries and model providers offer LLMs?\n","\n","Answer: \"\"\""],"metadata":{"id":"ui-CNWf5yO5V","executionInfo":{"status":"ok","timestamp":1694576351598,"user_tz":-330,"elapsed":3,"user":{"displayName":"Atindra Jayakar","userId":"03118470520503143272"}}},"execution_count":63,"outputs":[]},{"cell_type":"markdown","source":["LLM ususally suffer from hallucinations -> as model trys to give some output so tthe line \"do not know\" if cannnot helps with this problem.\n","\n","The format\n","* Context, Answer, Question (colon ':') helps the parsing agent find relavent parts"],"metadata":{"id":"wFtVkVLrjrL2"}},{"cell_type":"code","source":["flan_t5(prompt)"],"metadata":{"id":"Focf9AyYyavq","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1694576352217,"user_tz":-330,"elapsed":6,"user":{"displayName":"Atindra Jayakar","userId":"03118470520503143272"}},"outputId":"7df561ce-67fb-4ef3-c301-2ac2d8092bd0"},"execution_count":64,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"Hugging Face's transformers library, via OpenAI using the openai library, and via Cohere using the cohere library\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":64}]},{"cell_type":"markdown","source":["# **Prompt Template**"],"metadata":{"id":"8zSIby-FymIP"}},{"cell_type":"code","source":["from langchain import PromptTemplate\n","\n","template = \"\"\"Answer the question based on the context below. If the\n","question cannot be answered using the information provided answer\n","with \"I don't know\".\n","\n","Context: Large Language Models (LLMs) are the latest models used in NLP.\n","Their superior performance over smaller models has made them incredibly\n","useful for developers building NLP enabled applications. These models\n","can be accessed via Hugging Face's `transformers` library, via OpenAI\n","using the `openai` library, and via Cohere using the `cohere` library.\n","\n","Question: {query}\n","\n","Answer: \"\"\"\n","\n","prompt_template = PromptTemplate(\n","    input_variables=[\"query\"],\n","    template=template,\n",")"],"metadata":{"id":"LrOyFkwXynmf","executionInfo":{"status":"ok","timestamp":1694576352627,"user_tz":-330,"elapsed":3,"user":{"displayName":"Atindra Jayakar","userId":"03118470520503143272"}}},"execution_count":65,"outputs":[]},{"cell_type":"code","source":["print(\n","    prompt_template.format(\n","        query = \"Which libraries and model providers offer LLMs?\"\n","    )\n",") # this is what the api will see"],"metadata":{"id":"ILrR8YTKyryv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1694576352627,"user_tz":-330,"elapsed":2,"user":{"displayName":"Atindra Jayakar","userId":"03118470520503143272"}},"outputId":"aaaf6acb-ac9a-4a39-eb5f-b2e35271fb6a"},"execution_count":66,"outputs":[{"output_type":"stream","name":"stdout","text":["Answer the question based on the context below. If the\n","question cannot be answered using the information provided answer\n","with \"I don't know\".\n","\n","Context: Large Language Models (LLMs) are the latest models used in NLP.\n","Their superior performance over smaller models has made them incredibly\n","useful for developers building NLP enabled applications. These models\n","can be accessed via Hugging Face's `transformers` library, via OpenAI\n","using the `openai` library, and via Cohere using the `cohere` library.\n","\n","Question: Which libraries and model providers offer LLMs?\n","\n","Answer: \n"]}]},{"cell_type":"code","source":["print(flan_t5(prompt_template.format(\n","        query = \"Which libraries and model providers offer LLMs?\"\n","    )))"],"metadata":{"id":"l2Fj53hsywKW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1694576352981,"user_tz":-330,"elapsed":2,"user":{"displayName":"Atindra Jayakar","userId":"03118470520503143272"}},"outputId":"5c396188-4a33-4d0e-9aca-e3bbc357f849"},"execution_count":67,"outputs":[{"output_type":"stream","name":"stdout","text":["Hugging Face's transformers library, via OpenAI using the openai library, and via Cohere using the cohere library\n"]}]},{"cell_type":"markdown","source":["# **LLM Chain**"],"metadata":{"id":"Y2F3oPR_yhXY"}},{"cell_type":"markdown","source":["Chains are the core of LangChain. They are simply a chain of components, executed in a particular order.\n","\n","The simplest of these chains is the LLMChain. It works by taking a user's input, passing in to the first element in the chain — a PromptTemplate — to format the input into a particular prompt. The formatted prompt is then passed to the next (and final) element in the chain — a LLM."],"metadata":{"id":"8Z6NItDszt_1"}},{"cell_type":"code","source":["template = \"\"\"Question: {question}\n","Answer: Let's think step by step.\"\"\"\n","\n","prompt = PromptTemplate(template=template, input_variables=[\"question\"])"],"metadata":{"id":"K7n9P5t3y7ae","executionInfo":{"status":"ok","timestamp":1694576354866,"user_tz":-330,"elapsed":3,"user":{"displayName":"Atindra Jayakar","userId":"03118470520503143272"}}},"execution_count":68,"outputs":[]},{"cell_type":"code","source":["from langchain import LLMChain\n","\n","llm_chain = LLMChain(prompt=prompt, llm=flan_t5)"],"metadata":{"id":"quCFZaZmyXGz","executionInfo":{"status":"ok","timestamp":1694576355462,"user_tz":-330,"elapsed":1,"user":{"displayName":"Atindra Jayakar","userId":"03118470520503143272"}}},"execution_count":69,"outputs":[]},{"cell_type":"code","source":["question = \"Who is your creator?\"\n","print(question)\n","print(llm_chain.run(question))"],"metadata":{"id":"E5fKEjc-wPDa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1694576355837,"user_tz":-330,"elapsed":6,"user":{"displayName":"Atindra Jayakar","userId":"03118470520503143272"}},"outputId":"989355d7-c728-43e7-c743-c52b87ac2387"},"execution_count":70,"outputs":[{"output_type":"stream","name":"stdout","text":["Who is your creator?\n","Your creator is the person who made you. God is the creator of the universe. The universe is made up of matter and energy. The universe is made up of matter and energy. The answer: God.\n"]}]},{"cell_type":"code","source":["question = \"What is 2 * 5?\"\n","print(question)\n","print(llm_chain.run(question))"],"metadata":{"id":"Nr7209MZxJPD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1694576355837,"user_tz":-330,"elapsed":4,"user":{"displayName":"Atindra Jayakar","userId":"03118470520503143272"}},"outputId":"bf50bea1-25a4-40fb-e424-138b86a805b4"},"execution_count":71,"outputs":[{"output_type":"stream","name":"stdout","text":["What is 2 * 5?\n","2 * 5 is 5 because 2 is the product of 2 and 5. The answer: 5.\n"]}]},{"cell_type":"code","source":["# just by adding solve step by step  the model accuracy for maths is better (increaing temp:more than 0.4 or add this line if temp less than 0.5)\n","question = \"Solve step by step what is 2 * 5?\"\n","print(question)\n","print(llm_chain.run(question))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GHrxIGWenCiz","executionInfo":{"status":"ok","timestamp":1694576356206,"user_tz":-330,"elapsed":5,"user":{"displayName":"Atindra Jayakar","userId":"03118470520503143272"}},"outputId":"8046c19f-8eeb-4861-b0e2-6baaebc5b470"},"execution_count":72,"outputs":[{"output_type":"stream","name":"stdout","text":["Solve step by step what is 2 * 5?\n","2 * 5 is equal to 10 because 2 x 5 = 10. The answer: 10.\n"]}]},{"cell_type":"code","source":["question = \"What NFL team won the Super Bowl in the year Justin Beiber was born?\"\n","print(question)\n","print(llm_chain.run(question))"],"metadata":{"id":"a0emsTs3xid-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1694576357989,"user_tz":-330,"elapsed":5,"user":{"displayName":"Atindra Jayakar","userId":"03118470520503143272"}},"outputId":"565f983f-8a3f-4c2c-e7cc-885d50bf56b5"},"execution_count":73,"outputs":[{"output_type":"stream","name":"stdout","text":["What NFL team won the Super Bowl in the year Justin Beiber was born?\n","The Super Bowl was won by the New England Patriots in the year Justin Beiber was born. The Patriots won Super Bowl XXXV in the year Justin Beiber was born. The answer: the New England Patriots.\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"8U4yuQ_rm3oV","executionInfo":{"status":"ok","timestamp":1694576344299,"user_tz":-330,"elapsed":4,"user":{"displayName":"Atindra Jayakar","userId":"03118470520503143272"}}},"execution_count":61,"outputs":[]}]}